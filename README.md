# Automated News Aggregator ETL Pipeline

![Python](https://img.shields.io/badge/Python-3.9%2B-blue.svg)
![AWS](https://img.shields.io/badge/AWS%20S3-orange.svg)
![Pandas](https://img.shields.io/badge/Pandas-blueviolet.svg)

A simple, automated ETL (Extract, Transform, Load) pipeline that fetches technology news from a live API and stores it in AWS S3.

## üìã Project Description
This project demonstrates a fundamental cloud data engineering workflow. The pipeline performs the following actions:
1.  **Extract:** Fetches the latest technology news from the [NewsAPI.org](https://newsapi.org/) service.
2.  **Transform:** Uses the Pandas library to structure the raw JSON data into a clean, tabular format.
3.  **Load:** Saves the data locally as a `technology_news.csv` file and then uploads that file to a private bucket in AWS S3 for secure, scalable storage.

## üõ†Ô∏è Tech Stack
* **Language:** Python
* **Cloud Provider:** Amazon Web Services (AWS)
* **Key Libraries & Services:**
  * **Pandas:** For data manipulation.
  * **Requests:** For making HTTP requests to the API.
  * **Boto3:** The AWS SDK for Python, used for interacting with S3.
  * **AWS S3:** For object storage in the cloud.

## ‚öôÔ∏è Setup and Configuration

To run this project, you need to have your environment configured correctly.

1.  **Clone the repository:**
    ```bash
    git clone [https://github.com/ANUJ-GAUTAM26/news-aggregator-pipeline.git](https://github.com/ANUJ-GAUTAM26/news-aggregator-pipeline.git)
    cd news-aggregator-pipeline
    ```

2.  **Install Python dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

3.  **Configure AWS Credentials:**
    Ensure you have the AWS CLI installed and have configured your credentials with an IAM user that has S3 access.
    ```bash
    aws configure
    ```

4.  **Create a Configuration File:**
    Create a file named `config.ini` and add your credentials. This keeps your secrets out of the main script.
    ```ini
    [newsapi]
    api_key = YOUR_SECRET_API_KEY

    [aws]
    s3_bucket_name = your-unique-s3-bucket-name
    ```

## ‚ñ∂Ô∏è How to Run

Execute the main script from the root directory of the project:
```bash
python news_pipeline.py
